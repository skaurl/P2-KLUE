{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfNLv3imPqw-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Embedding, Reshape, Conv2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder; LE = LabelEncoder()\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def preprocessing_dataset(dataset, label_type):\n",
    "    label = []\n",
    "    for i in dataset[8]:\n",
    "        if i == 'blind':\n",
    "            label.append(100)\n",
    "        else:\n",
    "            label.append(label_type[i])\n",
    "    out_dataset = pd.DataFrame({'sentence':dataset[1],'entity_01':dataset[2],'entity_02':dataset[5],'label':label,})\n",
    "    return out_dataset\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "    with open('/content/drive/MyDrive/Colab Notebooks/label_type.pkl', 'rb') as f:\n",
    "        label_type = pickle.load(f)\n",
    "    dataset = pd.read_csv(dataset_dir, delimiter='\\t', header=None)\n",
    "    dataset = preprocessing_dataset(dataset, label_type)\n",
    "    return dataset\n",
    "\n",
    "def convert_to_ord(data):\n",
    "    try:\n",
    "        return [ord(xx) for xx in data]\n",
    "    except:\n",
    "        print(data)\n",
    "\n",
    "def conv2d_cnn():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=2**16, output_dim=output_dim, input_length=max_len))\n",
    "    model.add(Reshape((max_len, output_dim, 1), input_shape=(max_len, output_dim)))\n",
    "    model.add(Conv2D(filters=filters, kernel_size=(kernel_size, output_dim), strides=(1, 1), padding='valid'))\n",
    "    model.add(GlobalMaxPooling2D())\n",
    "\n",
    "    model.add(Dense(2**6))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(42))\n",
    "    model.add(Activation('softmax'))\n",
    "    adam = optimizers.Adam(lr=0.001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    early_stopping = EarlyStopping(patience=100)\n",
    "    history = model.fit(x_train, y_train, validation_split=0.1, epochs=epochs, batch_size=batch_size, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "    return model, history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    max_len = 128\n",
    "    output_dim = 200\n",
    "    filters = 400\n",
    "    kernel_size = 5\n",
    "    epochs = 2**6\n",
    "    batch_size = 2**10\n",
    "\n",
    "    dataset_path = r'/content/drive/MyDrive/Colab Notebooks/train.tsv'\n",
    "\n",
    "    dataset = load_data(dataset_path)\n",
    "\n",
    "    dataset['sentence'] = dataset['sentence'].map(convert_to_ord)\n",
    "    dataset['label'] = LE.fit_transform(dataset['label'])\n",
    "\n",
    "    data = sequence.pad_sequences(dataset['sentence'], maxlen=max_len)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, dataset['label'], test_size=0.1, random_state=42)\n",
    "\n",
    "    print('train_shape : {} / {}'.format(x_train.shape, y_train.shape))\n",
    "    print('test_shape : {} / {}'.format(x_test.shape, y_test.shape))\n",
    "\n",
    "    y_true = copy.deepcopy(y_test)\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "\n",
    "    model, history = conv2d_cnn()\n",
    "\n",
    "    scores = model.evaluate(x_test, y_test)\n",
    "    print(scores)\n",
    "    print(\"정확도: %.2f%%\" % (scores[1] * 100))\n",
    "\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    y_true = list(y_true)\n",
    "    y_pred = model.predict_classes(x_test)\n",
    "    y_pred = list(y_pred)\n",
    "\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(pd.crosstab(pd.Series(y_true), pd.Series(y_pred), rownames=['True'], colnames=['Predicted']))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNiIA4plF3e+4BLmRmQjtYO",
   "collapsed_sections": [],
   "mount_file_id": "1Y1Crs4bvhDEvi-yXGx8w3yVMGSUf3pL_",
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
