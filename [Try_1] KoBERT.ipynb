{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"P2-KLUE.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0RmcmqTvs1_T"},"source":["# Setting"]},{"cell_type":"markdown","metadata":{"id":"y5ZMzORj6Xxn"},"source":["라이브러리 다운로드"]},{"cell_type":"code","metadata":{"id":"oeI3L25s6XZP"},"source":["!pip install mxnet\n","!pip install gluonnlp pandas tqdm\n","!pip install sentencepiece\n","!pip install transformers==3\n","!pip install torch\n","!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rcI3nARqs9qg"},"source":["라이브러리 불러오기"]},{"cell_type":"code","metadata":{"id":"ETROhbNxsuXQ"},"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","import pandas as pd\n","import numpy as np\n","import re\n","import tarfile\n","import pickle as pickle\n","from tqdm import tqdm\n","from kobert.utils import get_tokenizer\n","from kobert.pytorch_kobert import get_pytorch_kobert_model\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KcobKDe9tAuQ"},"source":["GPU 설정"]},{"cell_type":"code","metadata":{"id":"i8v0khrlswNx"},"source":["device = torch.device(\"cuda:0\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0hZw_ITPtCgp"},"source":["kobert 불러오기"]},{"cell_type":"code","metadata":{"id":"nhsub2pBsx1q"},"source":["bertmodel, vocab = get_pytorch_kobert_model()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p9_lv7GMtE1_"},"source":["# Preprocessing"]},{"cell_type":"code","metadata":{"id":"5mr-nvcjOzLF"},"source":["def load_data(dataset_dir):\n","    with open('/opt/ml/input/data/label_type.pkl', 'rb') as f:\n","        label_type = pickle.load(f)\n","    dataset = pd.read_csv(dataset_dir, delimiter='\\t', header=None)\n","    dataset = preprocessing_dataset(dataset, label_type)\n","    return dataset\n","\n","def preprocessing_dataset(dataset, label_type):\n","    label = []\n","    for i in dataset[8]:\n","        if i == 'blind':\n","            label.append(100)\n","        else:\n","            label.append(label_type[i])\n","    out_dataset = pd.DataFrame({'sentence':dataset[1],'entity_01':dataset[2],'entity_02':dataset[5],'label':label,})\n","    return out_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xkI-E7PauxGq"},"source":["dataset_path = r\"/opt/ml/input/data/train/train.tsv\"\n","\n","dataset = load_data(dataset_path)\n","\n","dataset['sentence'] = dataset['entity_01'] + ' [SEP] ' + dataset['entity_02'] + ' [SEP] ' + dataset['sentence']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YIE_tnYq6AYL"},"source":["train, vali = train_test_split(dataset, test_size=0.2, random_state=42)\n","train[['sentence','label']].to_csv(\"/opt/ml/input/data/train/train_train.txt\", sep='\\t', index=False)\n","vali[['sentence','label']].to_csv(\"/opt/ml/input/data/train/train_vali.txt\", sep='\\t', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2tI-jupiCwpE"},"source":["dataset_train = nlp.data.TSVDataset(\"/opt/ml/input/data/train/train_train.txt\", field_indices=[0,1], num_discard_samples=1)\n","dataset_vali = nlp.data.TSVDataset(\"/opt/ml/input/data/train/train_vali.txt\", field_indices=[0,1], num_discard_samples=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ca54j41sN-0L"},"source":["tokenizer = get_tokenizer()\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eRRaHwF_C28c"},"source":["class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n","        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4BKznxZotPrl"},"source":["max_len = 128\n","batch_size = 32\n","warmup_ratio = 0.01\n","num_epochs = 20\n","max_grad_norm = 1\n","log_interval = 50\n","learning_rate = 5e-5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WtW5knVCC6ZC"},"source":["data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n","data_vali = BERTDataset(dataset_vali, 0, 1, tok, max_len, True, False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"spDs0h8tC7fX"},"source":["train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n","vali_dataloader = torch.utils.data.DataLoader(data_vali, batch_size=batch_size, num_workers=5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U0I1L7EVtShS"},"source":["# Classification"]},{"cell_type":"code","metadata":{"id":"eR9IqXuStUbL"},"source":["class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_size = 768,\n","                 num_classes = 42,\n","                 dr_rate=None,\n","                 params=None):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate \n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p=dr_rate)\n","    \n","    def gen_attention_mask(self, token_ids, valid_length):\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        return self.classifier(out)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"piJyyUoutWWt"},"source":["model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TqaRnWqwtXii"},"source":["no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YYExV_Uwqdpi"},"source":["class LabelSmoothingLoss(nn.Module):\n","    def __init__(self, classes=42, smoothing=0.0, dim=-1):\n","        super(LabelSmoothingLoss, self).__init__()\n","        self.confidence = 1.0 - smoothing\n","        self.smoothing = smoothing\n","        self.cls = classes\n","        self.dim = dim\n","\n","    def forward(self, pred, target):\n","        pred = pred.log_softmax(dim=self.dim)\n","        with torch.no_grad():\n","            true_dist = torch.zeros_like(pred)\n","            true_dist.fill_(self.smoothing / (self.cls - 1))\n","            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n","        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SvLPsHAMtYp4"},"source":["optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n","loss_fn = LabelSmoothingLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wJrYbrK5taVC"},"source":["t_total = len(train_dataloader) * num_epochs\n","warmup_step = int(t_total * warmup_ratio)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4PDk3f8ctasE"},"source":["scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7uxhVAqWtcbJ"},"source":["def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ASK6KHOTtd2H"},"source":["for e in range(num_epochs):\n","    train_acc = 0.0\n","    test_acc = 0.0\n","    best_acc = 0.0\n","    model.train()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(train_dataloader):\n","        optimizer.zero_grad()\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        loss = loss_fn(out, label)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","        optimizer.step()\n","        scheduler.step()\n","        train_acc += calc_accuracy(out, label)\n","        if batch_id % log_interval == 0:\n","            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n","    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n","    model.eval()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(vali_dataloader):\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length = valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        test_acc += calc_accuracy(out, label)\n","    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n","    if test_acc >= best_acc:\n","        best_acc = test_acc\n","        torch.save(model.state_dict(), \"/opt/ml/model/model_state_dict.pt\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h7ASgrTpfdZh"},"source":["# Predict"]},{"cell_type":"code","metadata":{"id":"Siuxwi6SdiTW"},"source":["dataset_path = r\"/opt/ml/input/data/test/test.tsv\"\n","\n","dataset = load_data(dataset_path)\n","\n","dataset['sentence'] = dataset['entity_01'] + ' [SEP] ' + dataset['entity_02'] + ' [SEP] ' + dataset['sentence']\n","\n","dataset[['sentence','label']].to_csv(\"/opt/ml/input/data/test/test.txt\", sep='\\t', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yPfoO4ym6AYU"},"source":["dataset_test = nlp.data.TSVDataset(\"/opt/ml/input/data/test/test.txt\", field_indices=[0,1], num_discard_samples=1)\n","\n","data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)\n","\n","test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g3TFf_YgtjDG"},"source":["model.load_state_dict(torch.load(\"/opt/ml/model/model_state_dict.pt\"))\n","\n","model.eval()\n","\n","Predict = []\n","\n","for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n","    token_ids = token_ids.long().to(device)\n","    segment_ids = segment_ids.long().to(device)\n","    valid_length = valid_length\n","    label = label.long().to(device)\n","    out = model(token_ids, valid_length, segment_ids)\n","    _, predict = torch.max(out,1)\n","    Predict.extend(predict.tolist())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_aV-Fgpffp4s"},"source":["output = pd.DataFrame(Predict, columns=['pred'])\n","output.to_csv('/opt/ml/result/submission.csv', index=False)"],"execution_count":null,"outputs":[]}]}